{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.book import FreqDist\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VA_accidents = pd.read_csv('VA_accidents_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VA_accidents.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## converting a list to text###\n",
    "text = VA_accidents['Description']\n",
    "# Function to convert   \n",
    "def listToString(s):  \n",
    "    \n",
    "    # initialize an empty string \n",
    "    str1 = \"\"  \n",
    "    \n",
    "    # traverse in the string   \n",
    "    for ele in s:  \n",
    "        str1 += ele   \n",
    "    \n",
    "    # return string   \n",
    "    return str1  \n",
    "        \n",
    "        \n",
    "# Driver code     \n",
    "text = (listToString(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word Tokenization\n",
    "words = word_tokenize(text)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Punctuations\n",
    "words = [word for word in words if word.isalpha()]\n",
    "## Converting to lowercase\n",
    "words = [x.lower() for x in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_sentence = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Remove stop words\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filter_sentence.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmaitizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "filter_sentence = [lemmatizer.lemmatize(word) for word in filter_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 25 words\n",
    "wordCount = {}\n",
    "for word in filter_sentence:\n",
    "    if word not in wordCount:\n",
    "        wordCount[word] = 1\n",
    "    else:\n",
    "        wordCount[word]+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('accident', 28858), ('due', 28061), ('blocked', 22655), ('exit', 22582), ('shoulder', 17415), ('hand', 16940), ('lane', 14159), ('closed', 11652), ('rd', 10428), ('northbound', 9436), ('southbound', 7582), ('eastbound', 6422), ('westbound', 5479), ('right', 5018), ('left', 4802), ('st', 4531), ('road', 3797), ('capital', 3396), ('hwy', 3314), ('beltway', 3022), ('ave', 2596), ('dr', 1621), ('pkwy', 1578), ('blvd', 1511), ('near', 1472), ('lee', 1341), ('broad', 1324), ('memorial', 1294), ('county', 1177), ('center', 1153), ('ramp', 1115), ('mill', 1090), ('bridge', 1053), ('old', 1031), ('washington', 1004), ('fairfax', 941), ('jefferson', 928), ('river', 894), ('hill', 845), ('davis', 819), ('richmond', 751), ('george', 717), ('sully', 706), ('tpke', 696), ('creek', 690), ('pike', 680), ('braddock', 654), ('byrd', 641), ('harry', 589), ('traffic', 568), ('beach', 549), ('main', 548), ('parham', 519), ('jackson', 510), ('leesburg', 508), ('hampton', 504), ('laburnum', 503), ('way', 500), ('james', 498), ('ln', 485), ('hull', 477), ('chain', 473), ('norfolk', 470), ('spring', 453), ('staple', 450), ('church', 448), ('va', 442), ('chamberlayne', 437), ('prince', 429), ('william', 429), ('john', 428), ('expy', 417), ('dulles', 415), ('ox', 394), ('loudoun', 386), ('gordon', 375), ('little', 372), ('brook', 362), ('ridge', 356), ('lorton', 349), ('mile', 346), ('forest', 345), ('madison', 344), ('king', 337), ('midlothian', 330), ('three', 329), ('valley', 327), ('new', 327), ('street', 326), ('mountain', 322), ('seminary', 320), ('courthouse', 319), ('mosby', 306), ('sudley', 304), ('dorn', 302), ('access', 300), ('mechanicsville', 300), ('two', 296), ('cary', 296), ('trl', 293), ('park', 285), ('bus', 278), ('charles', 276), ('hermitage', 273), ('duke', 268), ('town', 264), ('indian', 261), ('military', 248), ('patterson', 248), ('van', 247), ('bltwy', 247), ('nine', 247), ('arlington', 245), ('belvidere', 245), ('belmont', 244), ('monroe', 237), ('trailer', 235), ('toll', 234), ('leigh', 233), ('russell', 232), ('country', 229), ('springfield', 229), ('airport', 227), ('gaskin', 227), ('glebe', 226), ('franklin', 224), ('waxpool', 222), ('nutley', 221), ('tractor', 220), ('edsall', 220), ('run', 219), ('georgetown', 214), ('village', 211), ('williamsburg', 209), ('view', 206), ('sterling', 206), ('hamilton', 204), ('market', 203), ('henry', 200), ('kent', 200), ('hard', 198), ('wood', 197), ('brg', 197), ('franconia', 195), ('telegraph', 194), ('mount', 193), ('eisenhower', 193), ('marshall', 191), ('cedar', 189), ('hundred', 186), ('plank', 185), ('northampton', 184), ('monument', 183), ('dumfries', 183), ('commerce', 182), ('ashburn', 181), ('farm', 180), ('warrenton', 180), ('gap', 179), ('grove', 176), ('rock', 173), ('north', 170), ('pocahontas', 168), ('oak', 167), ('tavern', 167), ('south', 166), ('backlick', 165), ('glenside', 165), ('chopt', 163), ('patrick', 153), ('virginia', 153), ('terminal', 152), ('city', 151), ('point', 151), ('line', 149), ('roanoke', 149), ('gallows', 147), ('belt', 145), ('creighton', 143), ('newtown', 141), ('opitz', 137), ('meadow', 135), ('avenue', 134), ('lake', 134), ('hungary', 134), ('berlin', 133), ('g', 132), ('nuckols', 132), ('evergreen', 129), ('sycamore', 129), ('orange', 129), ('eustis', 129), ('colonial', 128), ('tidewater', 128), ('potomac', 126), ('landing', 126), ('involving', 126), ('maury', 125), ('tunl', 124), ('fort', 123)]\n"
     ]
    }
   ],
   "source": [
    "# Top 200 words\n",
    "frequency = nltk.FreqDist(filter_sentence)\n",
    "print(frequency.most_common(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NLP\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
